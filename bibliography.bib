@misc{ruder2017overview,
	title={An overview of gradient descent optimization algorithms}, 
	author={Sebastian Ruder},
	year={2017},
	eprint={1609.04747},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{chandra2019gradient,
	title={Gradient Descent: The Ultimate Optimizer}, 
	author={Kartik Chandra and Erik Meijer and Samantha Andow and Emilio Arroyo-Fang and Irene Dea and Johann George and Melissa Grueter and Basil Hosmer and Steffi Stumpos and Alanna Tempest and Shannon Yang},
	year={2019},
	eprint={1909.13371},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@online{KerasAdam,
	Title = {Adam},
	Url = {https://keras.io/api/optimizers/adam/},
	Urldate = {2021-09-06}
}

@online{EffNetEpochNumb,
	Title = {Transfer learning from pre-trained weights - Image classification via fine-tuning with EfficientNet},
	Url = {https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/#transfer-learning-from-pretrained-weights},
	Urldate = {2021-09-06}
}

@online{TFmetric,
	Title = {Module: tf.keras.metrics - TensorFlow Core v2.6.0},
	Url = {https://www.tensorflow.org/api_docs/python/tf/keras/metrics},
	Urldate = {2021-09-06}
}

@online{CateEncode,
	author = {Jason Brownlee},
	Title = {3 Ways to Encode Categorical Variables for Deep Learning},
	Url = {https://machinelearningmastery.com/how-to-prepare-categorical-data-for-deep-learning-in-python/},
	Urldate = {2021-09-06}
}

@online{LOEWE-Natur4.0,
	Title = {Natur 4.0 - kurz und bündig - Über uns - Natur 4.0 | Sensing Biodiversity - Philipps-Universität Marburg},
	Url = {https://www.uni-marburg.de/de/fb19/natur40/ueber-uns/natur4},
	Urldate = {2021-08-05}
}

@misc{russakovsky2015imagenet,
	title={ImageNet Large Scale Visual Recognition Challenge}, 
	author={Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
	year={2015},
	eprint={1409.0575},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@online{PapersWithCode-ImageNet,
	Title = {ImageNet Benchmark (Image Classification) | Papers With Code},
	Url = {https://paperswithcode.com/sota/image-classification-on-imagenet},
	Urldate = {2021-08-07}
}

@misc{dosovitskiy2021image,
	title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
	author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
	year={2021},
	eprint={2010.11929},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{tan2020efficientnet,
	title={EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}, 
	author={Mingxing Tan and Quoc V. Le},
	year={2020},
	eprint={1905.11946},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{he2015deep,
	title={Deep Residual Learning for Image Recognition}, 
	author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	year={2015},
	eprint={1512.03385},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@online{StanfordDAWNBench,
	Title = {Stanford DAWN Deep Learning Benchmark (DAWNBench) · ImageNet Training},
	Url = {https://dawn.cs.stanford.edu/benchmark/ImageNet/train.html},
	Urldate = {2021-08-08}
}

@online{BBVADeepLearning,
	author = {Ricardo Guerrero},
	Title = {Training Deep Learning Models On multi-GPus - BBVA Next Technologies},
	Url = {https://www.bbvanexttechnologies.com/blogs/training-deep-learning-models-on-multi-gpus/},
	Urldate = {2021-08-08}
}

@article{Schroff_2015,
	title={FaceNet: A unified embedding for face recognition and clustering},
	ISBN={9781467369640},
	url={http://dx.doi.org/10.1109/CVPR.2015.7298682},
	DOI={10.1109/cvpr.2015.7298682},
	journal={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	publisher={IEEE},
	author={Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
	year={2015},
	month={Jun}
}

@misc{codreanu2017scale,
	title={Scale out for large minibatch SGD: Residual network training on ImageNet-1K with improved accuracy and reduced time to train}, 
	author={Valeriu Codreanu and Damian Podareanu and Vikram Saletore},
	year={2017},
	eprint={1711.04291},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@online{ImageNet1000Classes,
	Title = {1000 Classification Categories - ImageNet Large Scale Visual Recognition Competition 2014 (ILSVRC2014)},
	Url = {https://image-net.org/challenges/LSVRC/2014/browse-synsets.php},
	Urldate = {2021-08-09}
}

@inbook{backpropapaper,
	author = {Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.},
	title = {Learning Internal Representations by Error Propagation},
	year = {1986},
	isbn = {026268053X},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
	booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations},
	pages = {318–362},
	numpages = {45}
}

@misc{boué2018deep,
	title={Deep learning for pedestrians: backpropagation in CNNs}, 
	author={Laurent Boué},
	year={2018},
	eprint={1811.11987},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@inbook{Goodfellow-et-al-2016,
	booktitle={Deep Learning},
	title={Back-Propagation and Other Diﬀerentiation Algorithms},
	author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	pages = {200–217},
	isbn = {0262035618},
	publisher={MIT Press},
	note={\url{http://www.deeplearningbook.org}},
	year={2016}
}

@online{DownloadImageNet,
	Title = {Download ImageNet Data - ImageNet},
	Url = {https://image-net.org/download.php},
	Urldate = {2021-08-11}
}

@article {PMID:14403678,
	Title = {Single unit activity in striate cortex of unrestrained cats},
	Author = {Hubel, David H.},
	DOI = {10.1113/jphysiol.1959.sp006238},
	Volume = {147},
	Month = {September},
	Year = {1959},
	Journal = {The Journal of physiology},
	ISSN = {0022-3751},
	Pages = {226—238},
	URL = {https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/14403678/?tool=EBI},
}

@article {PMID:14403679,
	Title = {Receptive fields of single neurones in the cat's striate cortex},
	Author = {Hubel, David H. and Wiesel, Torsten N.},
	DOI = {10.1113/jphysiol.1959.sp006308},
	Volume = {148},
	Month = {October},
	Year = {1959},
	Journal = {The Journal of physiology},
	ISSN = {0022-3751},
	Pages = {574—591},
	URL = {https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/14403679/?tool=EBI},
}

@article {PMID:14449617,
	Title = {Receptive fields, binocular interaction and functional architecture in the cat's visual cortex},
	Author = {Hubel, David H. and Wiesel, Torsten N.},
	DOI = {10.1113/jphysiol.1962.sp006837},
	Volume = {160},
	Month = {January},
	Year = {1962},
	Journal = {The Journal of physiology},
	Pages = {106—154},
	URL = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1359523/?tool=EBI},
}

@Inbook{Bergua2017,
	author="Bergua, Antonio",
	title="Visueller Kortex",
	bookTitle="Das menschliche Auge in Zahlen",
	year="2017",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="147--150",
	abstract="Der visuelle Kortex ist die Region des Gehirns, die f{\"u}r die Verarbeitung und Integration der visuellen Information verantwortlich ist. Er ist im Okzipitallappen lokalisiert, flankierend am Sulcus calcarinus. Der visuelle Kortex besteht aus dem prim{\"a}ren visuellen Kortex oder Brodmann-Areal 17 (V1), dem sekund{\"a}ren visuellen Kortex oder Brodmann-Areal 18 (V2) sowie anderen Arealen. Die afferenten Nervenimpulse aus den 1,5 Millionen Axonen der Retina, die {\"u}ber den Nervus opticus, Tractus opticus, das Corpus geniculatum laterale und die Radiatio optica verlaufen, erreichen am Ende dieser Bahn den visuellen Kortex, wo sie von ca. 200 Millionen Kortex-Neuronen verarbeitet werden. Der visuelle Kortex besitzt eine retinotopische Organisation, das hei{\ss}t jeder Punkt auf der Retina hat eine ganz exakte kortikale Repr{\"a}sentation. Die Fovea ist im visuellen Kortex {\"u}berdurchschnittlich repr{\"a}sentiert, um die hohe Aufl{\"o}sung des Sehens erm{\"o}glichen zu k{\"o}nnen.",
	isbn="978-3-662-47284-2",
	doi="10.1007/978-3-662-47284-2_27",
	url="https://doi.org/10.1007/978-3-662-47284-2_27"
}

@online{LocalReceptiveField,
	Title = {The Architecture of the Visual Cortex - Deep Computer Vision Using Convolutional Neural Networks - Programmer Sought},
	Url = {https://www.programmersought.com/article/65985444807/},
	Urldate = {2021-08-12}
}

@online{AufbauDNN,
	Title = {Types of Deep Learning Layers - What is Deep Learning? - Databricks},
	Url = {https://databricks.com/de/glossary/deep-learning},
	Urldate = {2021-08-12}
}

@article{Neocognitron1980,
	author="Kunihiko Fukushima",
	title="Neocognitron : A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
	journal="Biological Cybernetics",
	year="1980",
	volume="36",
	pages="193-202",
	URL="https://link.springer.com/article/10.1007%2FBF00344251",
	DOI="10.1007/BF00344251",
}

@article{Neocognitron1988,
	title = {Neocognitron: A hierarchical neural network capable of visual pattern recognition},
	journal = {Neural Networks},
	volume = {1},
	number = {2},
	pages = {119-130},
	year = {1988},
	issn = {0893-6080},
	doi = {https://doi.org/10.1016/0893-6080(88)90014-7},
	url = {https://www.sciencedirect.com/science/article/pii/0893608088900147},
	author = {Kunihiko Fukushima},
	abstract = {A neural network model for visual pattern recognition, called the “neocognitron,” was previously proposed by the author. In this paper, we discuss the mechanism of the model in detail. In order to demonstrate the ability of the neocognitron, we also discuss a pattern-recognition system which works with the mechanism of the neocognitron. The system has been implemented on a minicomputer and has been trained to recognize handwritten numerals. The neocognitron is a hierarchical network consisting of many layers of cells, and has variable connections between the cells in adjoining layers. It can acquire the ability to recognize patterns by learning, and can be trained to recognize any set of patterns. After finishing the process of learning, pattern recognition is performed on the basis of similarity in shape between patterns, and is not affected by deformation, nor by changes in size, nor by shifts in the position of the input patterns. In the hierarchical network of the neocognitron, local features of the input pattern are extracted by the cells of a lower stage, and they are gradually integrated into more global features. Finally, each cell of the highest stage integrates all the information of the input pattern, and responds only to one specific pattern. Thus, the response of the cells of the highest stage shows the final result of the pattern-recognition of the network. During this process of extracting and integrating features, errors in the relative position of local features are gradually tolerated. The operation of tolerating positional error a little at a time at each stage, rather than all in one step, plays an important role in endowing the network with an ability to recognize even distorted patterns.}
}

@online{neocognitron:scholarpediafig2,
	Title = {Principles of Deformation-Resistant Recognition - Neocognitron - Scholarpedia},
	Url = {http://www.scholarpedia.org/article/Neocognitron},
	Urldate = {2021-08-13}
}

@article{yannlecun1998,
	author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	journal={Proceedings of the IEEE}, 
	title={Gradient-based learning applied to document recognition}, 
	year={1998},
	volume={86},
	number={11},
	pages={2278-2324},
	doi={10.1109/5.726791}
}

@ARTICLE{yannlecun1989,
	author={LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
	journal={Neural Computation}, 
	title={Backpropagation Applied to Handwritten Zip Code Recognition}, 
	year={1989},
	volume={1},
	number={4},
	pages={541-551},
	doi={10.1162/neco.1989.1.4.541}
}

@article{10.1145/3065386,
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	title = {ImageNet Classification with Deep Convolutional Neural Networks},
	year = {2017},
	issue_date = {June 2017},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {60},
	number = {6},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million
	high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different
	classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%,
	respectively, which is considerably better than the previous state-of-the-art. The
	neural network, which has 60 million parameters and 650,000 neurons, consists of five
	convolutional layers, some of which are followed by max-pooling layers, and three
	fully connected layers with a final 1000-way softmax. To make training faster, we
	used non-saturating neurons and a very efficient GPU implementation of the convolution
	operation. To reduce overfitting in the fully connected layers we employed a recently
	developed regularization method called "dropout" that proved to be very effective.
	We also entered a variant of this model in the ILSVRC-2012 competition and achieved
	a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best
	entry.},
	journal = {Commun. ACM},
	month = may,
	pages = {84–90},
	numpages = {7}
}

@online{imagenet2012result,
	Title = {Analysis of ILSVRC2012 Results - ImageNet Challenge 2012 Analysis},
	Url = {https://image-net.org/challenges/LSVRC/2012/analysis/},
	Urldate = {2021-08-13}
}

@online{CS231nCNNarchitecture,
	Title = {Architecture Overview - Convolutional Neural Networks (CNNs / ConvNets) - CS231n Convolutional Neural Networks for Visual Recognition},
	Url = {https://cs231n.github.io/convolutional-networks/},
	Urldate = {2021-08-14}
}

@online{convlayer,
	Title = {Stacking Multiple Feature Maps - Deep Computer Vision Using Convolutional Neural Networks - Programmer Sought},
	Url = {https://www.programmersought.com/article/65985444807/},
	Urldate = {2021-08-15}
}

@book{10.5555/3378999,
	author = {Geron, Aurelien},
	title = {Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems},
	year = {2019},
	isbn = {1492032646},
	publisher = {O'Reilly Media, Inc.},
	edition = {2nd},
	abstract = {Through a series of recent breakthroughs, deep learning has boosted the entire field
	of machine learning. Now, even programmers who know close to nothing about this technology
	can use simple, efficient tools to implement programs capable of learning from data.
	This practical book shows you how. By using concrete examples, minimal theory, and
	two production-ready Python frameworks-Scikit-Learn and TensorFlow-author Aurelien
	Geron helps you gain an intuitive understanding of the concepts and tools for building
	intelligent systems. You'll learn a range of techniques, starting with simple linear
	regression and progressing to deep neural networks. With exercises in each chapter
	to help you apply what you've learned, all you need is programming experience to get
	started. Explore the machine learning landscape, particularly neural nets Use Scikit-Learn
	to track an example machine-learning project end-to-end Explore several training models,
	including support vector machines, decision trees, random forests, and ensemble methods
	Use the TensorFlow library to build and train neural nets Dive into neural net architectures,
	including convolutional nets, recurrent nets, and deep reinforcement learning Learn
	techniques for training and scaling deep neural nets}
}

@online{Softmax,
	author = {Wood, Thomas},
	Title = {Softmax Function Definition | DeepAI},
	Url = {https://deepai.org/machine-learning-glossary-and-terms/softmax-layer},
	Urldate = {2021-08-18}
}

@Article{Zschech2021,
	author={Zschech, Patrick
	and Sager, Christoph
	and Siebers, Philipp
	and Pertermann, Maik},
	title={Mit Computer Vision zur automatisierten Qualit{\"a}tssicherung in der industriellen Fertigung: Eine Fallstudie zur Klassifizierung von Fehlern in Solarzellen mittels Elektrolumineszenz-Bildern},
	journal={HMD Praxis der Wirtschaftsinformatik},
	year={2021},
	month={Apr},
	day={01},
	volume={58},
	number={2},
	pages={321-342},
	abstract={Die Qualit{\"a}tssicherung bei der Produktion von Solarzellen ist ein entscheidender Faktor, um langfristige Leistungsgarantien auf Solarpanels gew{\"a}hren zu k{\"o}nnen. Die vorliegende Arbeit leistet hierzu einen Beitrag zur automatisierten Fehlererkennung auf Wafern, indem Elektrolumineszenz-Bilder eines realen Herstellungsszenarios mithilfe von verschiedenen Computer-Vision-Modellen klassifiziert werden. Die Herausforderung besteht hierbei nicht nur darin, defekte Wafer von funktionsf{\"a}higen zu separieren, sondern gleichzeitig auch zwischen spezifischen Fehlerarten zu unterscheiden, w{\"a}hrend geringe Inferenzzeiten sicherzustellen sind. Zu diesem Zweck werden neben einfachen statistischen Modellen verschiedene Deep-Learning-Architekturen auf Basis von Convolutional Neural Networks (CNNs) verprobt und miteinander vergleichen. Ziel der Arbeit ist es, verschiedene Klassifizierungsans{\"a}tze unterschiedlicher Komplexit{\"a}t zu testen und auf ihre praktische Einsatzf{\"a}higkeit unter realen Bedingungen zu untersuchen. Die Fallstudie zeigt, dass je nach Situation unterschiedliche Modelle ihre Existenzberechtigung haben und in Kombination sehr gute Ergebnisse erzielen. So lassen sich bereits mit statistischen Modellen und einfachen CNN-Varianten zuverl{\"a}ssige Aussagen mit Genauigkeiten von {\"u}ber 99 {\%} bei Fehlertypen einfacher bis mittlerer Erkennbarkeit realisieren. Werden die Fehlerbilder demgegen{\"u}ber diffuser und soll die Nachvollziehbarkeit der Ergebnisse durch positionsgenaue Lokalisierung von Fehlerobjekten gew{\"a}hrleistet werden, sind fortgeschrittenere Ans{\"a}tze auf Basis sogenannter Region-Proposal-Netzwerke erforderlich, die allerdings auch mit einem erh{\"o}hten Labeling-Aufwand beim Annotieren der Fehlerobjekte einhergehen. Da die Umsetzung s{\"a}mtlicher Modelle ausschlie{\ss}lich auf Open Source Tools wie zum Beispiel TensorFlow, Keras und OpenCV basiert, demonstriert die Fallstudie zudem, welche M{\"o}glichkeiten durch frei verf{\"u}gbare L{\"o}sungen im Bereich von Computer Vision geboten werden.},
	issn={2198-2775},
	doi={10.1365/s40702-020-00641-8},
	url={https://doi.org/10.1365/s40702-020-00641-8}
}

@online{ResNet200_101_PaperWithCodes,
	Title = {Parametric Contrastive Learning | Papers With Code},
	Url = {https://paperswithcode.com/paper/parametric-contrastive-learning},
	Urldate = {2021-08-18}
}

@misc{cui2021parametric,
	title={Parametric Contrastive Learning}, 
	author={Jiequan Cui and Zhisheng Zhong and Shu Liu and Bei Yu and Jiaya Jia},
	year={2021},
	eprint={2107.12028},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{zagoruyko2017wide,
	title={Wide Residual Networks}, 
	author={Sergey Zagoruyko and Nikos Komodakis},
	year={2017},
	eprint={1605.07146},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@online{VanishingGradientProblem,
	author = {Yash Bohra},
	Title = {Vanishing and Exploding Gradients in Deep Neural Networks},
	Url = {https://www.analyticsvidhya.com/blog/2021/06/the-challenge-of-vanishing-exploding-gradients-in-deep-neural-networks/},
	Urldate = {2021-08-18}
}

@misc{howard2017mobilenets,
	title={MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications}, 
	author={Andrew G. Howard and Menglong Zhu and Bo Chen and Dmitry Kalenichenko and Weijun Wang and Tobias Weyand and Marco Andreetto and Hartwig Adam},
	year={2017},
	eprint={1704.04861},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{huang2019gpipe,
	title={GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism}, 
	author={Yanping Huang and Youlong Cheng and Ankur Bapna and Orhan Firat and Mia Xu Chen and Dehao Chen and HyoukJoong Lee and Jiquan Ngiam and Quoc V. Le and Yonghui Wu and Zhifeng Chen},
	year={2019},
	eprint={1811.06965},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{liashchynskyi2019grid,
	title={Grid Search, Random Search, Genetic Algorithm: A Big Comparison for NAS}, 
	author={Petro Liashchynskyi and Pavlo Liashchynskyi},
	year={2019},
	eprint={1912.06059},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{tan2019mnasnet,
	title={MnasNet: Platform-Aware Neural Architecture Search for Mobile}, 
	author={Mingxing Tan and Bo Chen and Ruoming Pang and Vijay Vasudevan and Mark Sandler and Andrew Howard and Quoc V. Le},
	year={2019},
	eprint={1807.11626},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{li2017webvision,
	title={WebVision Database: Visual Learning and Understanding from Web Data}, 
	author={Wen Li and Limin Wang and Wei Li and Eirikur Agustsson and Luc Van Gool},
	year={2017},
	eprint={1708.02862},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{vanhorn2018inaturalist,
	title={The iNaturalist Species Classification and Detection Dataset}, 
	author={Grant Van Horn and Oisin Mac Aodha and Yang Song and Yin Cui and Chen Sun and Alex Shepard and Hartwig Adam and Pietro Perona and Serge Belongie},
	year={2018},
	eprint={1707.06642},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{vanhorn2021benchmarking,
	title={Benchmarking Representation Learning for Natural World Image Collections}, 
	author={Grant Van Horn and Elijah Cole and Sara Beery and Kimberly Wilber and Serge Belongie and Oisin Mac Aodha},
	year={2021},
	eprint={2103.16483},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@online{MLDatasetsPWC,
	Title = {Machine Learning Datasets | Papers With Code},
	Url = {https://paperswithcode.com/datasets?task=image-classification&mod=images&page=1&v=th},
	Urldate = {2021-08-20}
}

@online{KerasRef,
	Title = {keras-team/keras-applications: Reference implementations of popular deep learning models.},
	Url = {https://github.com/keras-team/keras-applications},
	Urldate = {2021-08-20}
}

@misc{simonyan2015deep,
	title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
	author={Karen Simonyan and Andrew Zisserman},
	year={2015},
	eprint={1409.1556},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{zoph2018learning,
	title={Learning Transferable Architectures for Scalable Image Recognition}, 
	author={Barret Zoph and Vijay Vasudevan and Jonathon Shlens and Quoc V. Le},
	year={2018},
	eprint={1707.07012},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{sandler2019mobilenetv2,
	title={MobileNetV2: Inverted Residuals and Linear Bottlenecks}, 
	author={Mark Sandler and Andrew Howard and Menglong Zhu and Andrey Zhmoginov and Liang-Chieh Chen},
	year={2019},
	eprint={1801.04381},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{howard2019searching,
	title={Searching for MobileNetV3}, 
	author={Andrew Howard and Mark Sandler and Grace Chu and Liang-Chieh Chen and Bo Chen and Mingxing Tan and Weijun Wang and Yukun Zhu and Ruoming Pang and Vijay Vasudevan and Quoc V. Le and Hartwig Adam},
	year={2019},
	eprint={1905.02244},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{he2016identity,
	title={Identity Mappings in Deep Residual Networks}, 
	author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	year={2016},
	eprint={1603.05027},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{huang2018densely,
	title={Densely Connected Convolutional Networks}, 
	author={Gao Huang and Zhuang Liu and Laurens van der Maaten and Kilian Q. Weinberger},
	year={2018},
	eprint={1608.06993},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{szegedy2015rethinking,
	title={Rethinking the Inception Architecture for Computer Vision}, 
	author={Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jonathon Shlens and Zbigniew Wojna},
	year={2015},
	eprint={1512.00567},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{xie2017aggregated,
	title={Aggregated Residual Transformations for Deep Neural Networks}, 
	author={Saining Xie and Ross Girshick and Piotr Dollár and Zhuowen Tu and Kaiming He},
	year={2017},
	eprint={1611.05431},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{chollet2017xception,
	title={Xception: Deep Learning with Depthwise Separable Convolutions}, 
	author={François Chollet},
	year={2017},
	eprint={1610.02357},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{szegedy2016inceptionv4,
	title={Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}, 
	author={Christian Szegedy and Sergey Ioffe and Vincent Vanhoucke and Alex Alemi},
	year={2016},
	eprint={1602.07261},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{real2019regularized,
	title={Regularized Evolution for Image Classifier Architecture Search}, 
	author={Esteban Real and Alok Aggarwal and Yanping Huang and Quoc V Le},
	year={2019},
	eprint={1802.01548},
	archivePrefix={arXiv},
	primaryClass={cs.NE}
}

@misc{cubuk2019autoaugment,
	title={AutoAugment: Learning Augmentation Policies from Data}, 
	author={Ekin D. Cubuk and Barret Zoph and Dandelion Mane and Vijay Vasudevan and Quoc V. Le},
	year={2019},
	eprint={1805.09501},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{salman2019overfitting,
	title={Overfitting Mechanism and Avoidance in Deep Neural Networks}, 
	author={Shaeke Salman and Xiuwen Liu},
	year={2019},
	eprint={1901.06566},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{zhai2021scaling,
	title={Scaling Vision Transformers}, 
	author={Xiaohua Zhai and Alexander Kolesnikov and Neil Houlsby and Lucas Beyer},
	year={2021},
	eprint={2106.04560},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{pham2021meta,
	title={Meta Pseudo Labels}, 
	author={Hieu Pham and Zihang Dai and Qizhe Xie and Minh-Thang Luong and Quoc V. Le},
	year={2021},
	eprint={2003.10580},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{kolesnikov2020big,
	title={Big Transfer (BiT): General Visual Representation Learning}, 
	author={Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Joan Puigcerver and Jessica Yung and Sylvain Gelly and Neil Houlsby},
	year={2020},
	eprint={1912.11370},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@misc{ridnik2021imagenet21k,
	title={ImageNet-21K Pretraining for the Masses}, 
	author={Tal Ridnik and Emanuel Ben-Baruch and Asaf Noy and Lihi Zelnik-Manor},
	year={2021},
	eprint={2104.10972},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@online{ViThuggingface,
	Title = {Vision Transformer (ViT) - transformers 4.7.0 documentation},
	Url = {https://huggingface.co/transformers/model_doc/vit.html},
	Urldate = {2021-08-21}
}

@online{ImageNet21Kclassname,
	Title = {ImageNet-21K WordNet Lemmas},
	Url = {https://storage.googleapis.com/bit_models/imagenet21k_wordnet_lemmas.txt},
	Urldate = {2021-08-21}
}

@online{KerasEffNetAPI,
	Title = {EfficientNet B0 to B7},
	Url = {https://keras.io/api/applications/efficientnet/},
	Urldate = {2021-08-21}
}

@article{beery2019efficient,
	title={Efficient Pipeline for Camera Trap Image Review},
	author={Beery, Sara and Morris, Dan and Yang, Siyu},
	journal={arXiv preprint arXiv:1907.06772},
	year={2019}
}

@online{AIforEarth,
	Title = {AI for Earth - Microsoft AI},
	Url = {https://www.microsoft.com/en-us/ai/ai-for-earth},
	Urldate = {2021-08-22}
}

@misc{ren2016faster,
	title={Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}, 
	author={Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},
	year={2016},
	eprint={1506.01497},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@online{AIforEarth,
	Title = {AI for Earth - Microsoft AI},
	Url = {https://www.microsoft.com/en-us/ai/ai-for-earth},
	Urldate = {2021-08-22}
}

@online{CameraTrapSurvey,
	author = {Dan Morris},
	Title = {Overview | Camera Trap ML Survey},
	Url = {https://agentmorris.github.io/camera-trap-ml-survey/#microsoft-ai-for-earth-camera-trap-api},
	Urldate = {2021-08-22}
}

@online{iNatObservations,
	Title = {Observations - iNaturalist},
	Url = {https://www.inaturalist.org/observations},
	Urldate = {2021-08-23}
}

@online{pyinat,
	Title = {pyinaturalist - PyPI},
	Url = {https://pypi.org/project/pyinaturalist/},
	Urldate = {2021-08-24}
}

@book{imholt2021langfristige,
	title={Langfristige Populationsentwicklung krankheits{\"u}bertragender Nagetiere: Interaktion von Klimawandel, Landnutzung und Biodiversit{\"a}t : Abschlussbericht},
	author={Imholt, C. and Jacob, J. and Schl{\"o}telburg, A. and Geduhn, A. and Deutschland Umweltbundesamt and Institut f{\"u}r Pflanzenschutz in Gartenbau und Forst},
	series={Climate change},
	url={https://books.google.de/books?id=eHSWzgEACAAJ},
	year={2021},
	publisher={Umweltbundesamt}
}

@online{snapshotWCS,
	Title = {About Snapshot Wisconsin - Zooniverse},
	Url = {https://www.zooniverse.org/projects/zooniverse/snapshot-wisconsin/about/research},
	Urldate = {2021-08-24}
}

@online{iNatAPI,
	Title = {iNaturalist API},
	Url = {https://api.inaturalist.org/v1/docs/},
	Urldate = {2021-08-26}
}

@online{WCSOrigin,
	author = {Willi, Marco},
	Title = {Camera Trap Images used in "Identifying Animal Species in Camera Trap Images using Deep Learning and Citizen Science"},
	Url = {https://conservancy.umn.edu/handle/11299/199819},
	Urldate = {2021-08-27}
}

@online{WCSBaseURL,
	Title = {Snapshot Wisconsin Talk - Zooniverse},
	Url = {https://www.zooniverse.org/projects/zooniverse/snapshot-wisconsin/talk/subjects/%7Bid%7D},
	Urldate = {2021-08-27}
}

@article{SHAHINFAR2020101085,
	title = {“How many images do I need?” Understanding how sample size per class affects deep learning model performance metrics for balanced designs in autonomous wildlife monitoring},
	journal = {Ecological Informatics},
	volume = {57},
	pages = {101085},
	year = {2020},
	issn = {1574-9541},
	doi = {https://doi.org/10.1016/j.ecoinf.2020.101085},
	url = {https://www.sciencedirect.com/science/article/pii/S1574954120300352},
	author = {Saleh Shahinfar and Paul Meek and Greg Falzon},
	keywords = {Camera traps, Deep learning, Ecological informatics, Generalised additive models, Learning curves, Predictive modelling, Wildlife},
	abstract = {Deep learning (DL) algorithms are the state of the art in automated classification of wildlife camera trap images. The challenge is that the ecologist cannot know in advance how many images per species they need to collect for model training in order to achieve their desired classification accuracy. In fact there is limited empirical evidence in the context of camera trapping to demonstrate that increasing sample size will lead to improved accuracy. In this study we explore in depth the issues of deep learning model performance for progressively increasing per class (species) sample sizes. We also provide ecologists with an approximation formula to estimate how many images per animal species they need for certain accuracy level a priori. This will help ecologists for optimal allocation of resources, work and efficient study design. In order to investigate the effect of number of training images; seven training sets with 10, 20, 50, 150, 500, 1000 images per class were designed. Six deep learning architectures namely ResNet-18, ResNet-50, ResNet-152, DnsNet-121, DnsNet-161, and DnsNet-201 were trained and tested on a common exclusive testing set of 250 images per class. The whole experiment was repeated on three similar datasets from Australia, Africa and North America and the results were compared. Simple regression equations for use by practitioners to approximate model performance metrics are provided. Generalizes additive models (GAM) are shown to be effective in modelling DL performance metrics based on the number of training images per class, tuning scheme and dataset. Overall, our trained models classified images with 0.94 accuracy (ACC), 0.73 precision (PRC), 0.72 true positive rate (TPR), and 0.03 false positive rate (FPR). Variation in model performance metrics among datasets, species and deep learning architectures exist and are shown distinctively in the discussion section. The ordinary least squares regression models explained 57%, 54%, 52%, and 34% of expected variation of ACC, PRC, TPR, and FPR according to number of images available for training. Generalised additive models explained 77%, 69%, 70%, and 53% of deviance for ACC, PRC, TPR, and FPR respectively. Predictive models were developed linking number of training images per class, model, dataset to performance metrics. The ordinary least squares regression and Generalised additive models developed provides a practical toolbox to estimate model performance with respect to different numbers of training images.}
}

@online{Underfitting,
	author = {Christian Versloot},
	Title = {How to check if your Deep Learning model is underfitting or overfitting? – MachineCurve},
	Url = {https://www.machinecurve.com/index.php/2020/12/01/how-to-check-if-your-deep-learning-model-is-underfitting-or-overfitting/},
	Urldate = {2021-08-28}
}

@online{EffNetBaseModelResolution,
	Title = {Keras implementation of EfficientNet - Image classification via fine-tuning with EfficientNet},
	Url = {https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/#keras-implementation-of-efficientnet},
	Urldate = {2021-08-29}
}

@Article{Shorten2019,
	author={Shorten, Connor
	and Khoshgoftaar, Taghi M.},
	title={A survey on Image Data Augmentation for Deep Learning},
	journal={Journal of Big Data},
	year={2019},
	month={Jul},
	day={06},
	volume={6},
	number={1},
	pages={60},
	abstract={Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	issn={2196-1115},
	doi={10.1186/s40537-019-0197-0},
	url={https://doi.org/10.1186/s40537-019-0197-0}
}

@online{CrossEntropyLoss,
	author = {Jason Brownlee},
	Title = {A Gentle Introduction to Cross-Entropy for Machine Learning},
	Url = {https://machinelearningmastery.com/cross-entropy-for-machine-learning/},
	Urldate = {2021-08-30}
}

@misc{xue2021wider,
	title={Go Wider Instead of Deeper}, 
	author={Fuzhao Xue and Ziji Shi and Futao Wei and Yuxuan Lou and Yong Liu and Yang You},
	year={2021},
	eprint={2107.11817},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@online{Novustat,
	Title = {So vermeidet man Overfitting und Underfitting! | Novustat},
	Url = {https://novustat.com/statistik-blog/overfitting-und-underfitting.html},
	Urldate = {2021-08-31}
}

@online{nvidiacomputecapa,
	Title = {Compute Capability - Programming Model - Programming Guide :: CUDA Toolkit Documentation},
	Url = {https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability},
	Urldate = {2021-09-02}
}

@online{gtxtitanx,
	Title = {NVIDIA GeForce GTX TITAN X specs},
	Url = {https://www.gpuzoo.com/GPU-NVIDIA/GeForce_GTX_TITAN_X.html},
	Urldate = {2021-09-02}
}

@online{gtx580,
	Title = {NVIDIA GeForce GTX 580 specs},
	Url = {https://www.gpuzoo.com/GPU-NVIDIA/GeForce_GTX_580.html},
	Urldate = {2021-09-02}
}